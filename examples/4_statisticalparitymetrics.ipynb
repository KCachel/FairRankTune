{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3J4PihlhCRNJVFRzomw7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCachel/FairRankTune/blob/main/examples/4_statisticalparitymetrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "aPfbFCZkmYxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install [FairRankTune](https://https://github.com/KCachel/FairRankTune)."
      ],
      "metadata": {
        "id": "tfF1MsSEjhbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FairRankTune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYw3v4_jx74",
        "outputId": "4a052eea-f1e6-4adf-e059-690daa7b4467"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: FairRankTune in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to import FairRankTune along with some other packages."
      ],
      "metadata": {
        "id": "UH63dmzEkAZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import FairRankTune as frt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from FairRankTune import RankTune, Metrics, Rankers"
      ],
      "metadata": {
        "id": "FyEGSwpWj9CZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics (Evaluating Rankings for Fairness)\n",
        "The [Metrics](https://https://kcachel.github.io/FairRankTune/Metrics/) module contains several fairness metrics for assessing ranked lists. Each metric evaluates the ranking(s) in the passed `ranking_df` parameter (a Pandas dataframe).\n",
        "\n",
        "In this overview, we will demonstrate the statistical parity metrics.  Statistical Parity is a sub-tupe of group fairness, asks for groups to receive a proportional share of the positive outcome. In ranking(s) the positive outcome can be the exposure or attention of the viewer or a share of top-ranked positions. Statistical Parity is also known as Demographic Parity and explicitly does not use relevance scores. For example, we might want to know if groups receive comporable amounts of exposure regardless of the relevance scores associated with items.\n"
      ],
      "metadata": {
        "id": "QAYDtiE9kxXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular Metrics\n",
        "A key functionality of the  `Metrics` library in `FairRankTune`  is providing toolkit users multiple choices for how to calculate a given top-level fairness metric. For instance, for group exposure EXP a statistical parity metric,  `Metrics` offers seven ways of calculating a top-level exposure metric (e.g., min-max ratios, max absolute difference, L-2 norms of per-group exposures, etc.).\n",
        "\n",
        "\n",
        "Below are the formulas supported for combining per-group style metrics. In the formulas $V = [V_{1}, ..., V_{g}$] is an array of per-group metrics and $G$ is the number of groups. The `combo` variable is used directly in the function call. Depending on the formula used for aggregating per-group metrics the range of the given fairness metric varies. The range and its corresponding \"most fair\" value is provided in the table.\n",
        "\n",
        "| **Combo Variable in ```FairRankTune```** | **Formula** | **Range** | **Most Fair** |\n",
        "|---|:---:|:---:|:---:|\n",
        "| ```MinMaxRatio``` |  $min_{g} V / max_{g} V$ | [0,1] | 1 |\n",
        "| ```MaxMinRatio``` |  $max_{g} V / min_{g} V$ | [1, $\\infty$] | 1 |\n",
        "| ```MaxMinDiff``` |  $max_{g} V - min_{g} V$ |  [0,1] | 0 |\n",
        "| ```MaxAbsDiff``` | $max_{g} \\mid V - V_{mean} \\mid$ |  [0, $\\infty$] | 0 |\n",
        "| ```MeanAbsDev``` | $\\frac{1}{G} \\sum_{g} \\mid V - V_{mean}\\mid$ | [0, $\\infty$] | 0 |\n",
        "| ```LTwo```| $\\lVert V \\rVert_2^2$ | [0, $\\infty$] | 0 |\n",
        "|  ```Variance``` |  $\\frac{1}{G - 1} \\sum_{g} (V_{g} - V_{mean})^2$ | [0, $\\infty$] | 0 |\n",
        "\n",
        "\n",
        "Here, we have split the statistical parity metrics into the modular metrics, and the metrics that are mot meta-metric composable."
      ],
      "metadata": {
        "id": "gBJa9RbdKG4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Parity Modular Metrics"
      ],
      "metadata": {
        "id": "EPlcXAV0KmiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Exposure EXP\n",
        "[EXP](https://kcachel.github.io/FairRankTune/Metrics/#group-exposure-exp) compares the average exposures of groups in the ranking(s) and does not consider relevances or scores associate with items. It aligns with the fairness concept of statistical parity. The per-group metric is the group average exposure, whereby the exposure of item $x_i$ in ranking $\\tau$ is $exposure(\\tau,x_i) = 1 / log_2(\\tau(x_i)+1))$ and the average exposure for group $g_j$ is $avgexp(\\tau,g_j) = \\sum_{\\forall x \\in g_{j}}exposure(\\tau,x_i)/|g_{j}|$. The range of EXP and its \"most fair\" value depends on the `combo` variable."
      ],
      "metadata": {
        "id": "m1TCNDU-LEu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below we calculate EXP across all aggregation functions. We can see that the average exposures for 'M' (men) and 'W' (women) are always the same, but the EXP value varies depenging on the meta-metric used."
      ],
      "metadata": {
        "id": "RddrplawN6KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df = pd.DataFrame([\"Joe\", \"Jack\", \"Nick\", \"David\", \"Mark\", \"Josh\", \"Dave\",\n",
        "                          \"Bella\", \"Heidi\", \"Amy\"])\n",
        "item_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\", Mark= \"M\", Josh= \"M\", Dave= \"M\", Jack= \"M\", Nick= \"M\")\n",
        "\n",
        "#Calculate EXP\n",
        "EXPMaxMinDiff, exps_MaxMinDiff = frt.Metrics.EXP(ranking_df, item_group_dict, 'MaxMinDiff')\n",
        "print(\"EXP (MaxMinDiff): \", EXPMaxMinDiff, \"avg_exposures: \", exps_MaxMinDiff)\n",
        "\n",
        "EXPMinMaxRatio, exps_MinMaxRatio = frt.Metrics.EXP(ranking_df, item_group_dict, 'MinMaxRatio')\n",
        "print(\"EXP (MinMaxRatio): \", EXPMinMaxRatio, \"avg_exposures: \", exps_MinMaxRatio)\n",
        "\n",
        "EXPMaxMinRatio, exps_MaxMinRatio = frt.Metrics.EXP(ranking_df, item_group_dict, 'MaxMinRatio')\n",
        "print(\"EXP (MaxMinRatio): \", EXPMaxMinRatio, \"avg_exposures: \", exps_MaxMinRatio)\n",
        "\n",
        "EXPMaxAbsDiff, exps_MaxAbsDiff = frt.Metrics.EXP(ranking_df, item_group_dict, 'MaxAbsDiff')\n",
        "print(\"EXP (MaxAbsDiff): \", EXPMaxAbsDiff, \"avg_exposures: \", exps_MaxAbsDiff)\n",
        "\n",
        "\n",
        "EXPMeanAbsDev, exps_MeanAbsDev = frt.Metrics.EXP(ranking_df, item_group_dict, 'MeanAbsDev')\n",
        "print(\"EXP (MeanAbsDev): \", EXPMeanAbsDev, \"avg_exposures: \", exps_MeanAbsDev)\n",
        "\n",
        "\n",
        "\n",
        "EXPLTwo, exps_LTwo = frt.Metrics.EXP(ranking_df, item_group_dict, 'LTwo')\n",
        "print(\"EXP (LTwo): \", EXPLTwo, \"avg_exposures: \", exps_LTwo)\n",
        "\n",
        "EXPVariance, exps_Variance = frt.Metrics.EXP(ranking_df, item_group_dict, 'Variance')\n",
        "print(\"EXP (Variance): \", EXPVariance, \"avg_exposures: \", exps_Variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eENDsfZcLTPC",
        "outputId": "e91a7e88-abb1-454c-f126-d5ce227f15ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXP (MaxMinDiff):  0.21786100126614577 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n",
            "EXP (MinMaxRatio):  0.5808061682084833 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n",
            "EXP (MaxMinRatio):  1.721744800136222 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n",
            "EXP (MaxAbsDiff):  0.10893050063307291 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n",
            "EXP (MeanAbsDev):  0.10893050063307289 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n",
            "EXP (LTwo):  0.6010143587670008 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n",
            "EXP (Variance):  0.011865853968171892 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Weighted Rank Fairness (AWRF)\n",
        "\n",
        "[AWRF](https://kcachel.github.io/FairRankTune/Metrics/#attention-weighted-rank-fairness-awrf) compares the average attention of groups in the ranking(s)and does not consider relevances or scores associate with items. It aligns with the fairness concept of statistical parity.  Attention compared to EXP uses a geometric discount on the \"attention\" assigned to positions in a ranking. The per-group metric is the group average attention,\n",
        "whereby the attention score for item $x_i$ in ranking $\\tau$ as $attention(\\tau,x_i) = 100 \\times (1 - p) ^{(\\tau(x_i) -1)} \\times p$, where $p$ is a parameter representing the proportion of attention received by the first (top) ranked item.  The range of AWRF and its \"most fair\" value depends on the `combo` variable.\n",
        "\n",
        "In the example below we calculate AWRF across all aggregation functions. We can see that the average exposures for 'M' (men) and 'W' (women) are always the same, but the AWRF value varies depenging on the meta-metric used.\n"
      ],
      "metadata": {
        "id": "jBYszXXOOSrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df = pd.DataFrame([\"Joe\", \"Jack\", \"Nick\", \"David\", \"Mark\", \"Josh\", \"Dave\",\n",
        "                          \"Bella\", \"Heidi\", \"Amy\"])\n",
        "item_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\", Mark= \"M\", Josh= \"M\", Dave= \"M\", Jack= \"M\", Nick= \"M\")\n",
        "\n",
        "#Calculate AWRF\n",
        "p = .1 #paramater representing the proportion of attention received by the first postion\n",
        "AWRFMaxMinDiff, AWRFs_MaxMinDiff = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'MaxMinDiff')\n",
        "print(\"AWRF (MaxMinDiff): \", AWRFMaxMinDiff, \"avg_attention: \", AWRFs_MaxMinDiff)\n",
        "\n",
        "AWRFMinMaxRatio, AWRFs_MinMaxRatio = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'MinMaxRatio')\n",
        "print(\"AWRF (MinMaxRatio): \", AWRFMinMaxRatio, \"avg_attention: \", AWRFs_MinMaxRatio)\n",
        "\n",
        "AWRFMaxMinRatio, AWRFs_MaxMinRatio = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'MaxMinRatio')\n",
        "print(\"AWRF (MaxMinRatio): \", AWRFMaxMinRatio, \"avg_attention: \", AWRFs_MaxMinRatio)\n",
        "\n",
        "AWRFMaxAbsDiff, AWRFs_MaxAbsDiff = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'MaxAbsDiff')\n",
        "print(\"AWRF (MaxAbsDiff): \", AWRFMaxAbsDiff, \"avg_attention: \", AWRFs_MaxAbsDiff)\n",
        "\n",
        "AWRFMeanAbsDev, AWRFs_MeanAbsDev = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'MeanAbsDev')\n",
        "print(\"AWRF (MeanAbsDev): \", AWRFMeanAbsDev, \"avg_attention: \", AWRFs_MeanAbsDev)\n",
        "\n",
        "AWRFLTwo, AWRFs_LTwo = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'LTwo')\n",
        "print(\"AWRF (LTwo): \", AWRFLTwo, \"avg_attention: \", AWRFs_LTwo)\n",
        "\n",
        "AWRFVariance, AWRFs_Variance = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'Variance')\n",
        "print(\"AWRF (Variance): \", AWRFVariance, \"avg_attention: \", AWRFs_Variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtBbuPBgOrsu",
        "outputId": "93f77cbb-c398-43b5-cf06-0aaa9b9e1778"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AWRF (MaxMinDiff):  3.132286098571428 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n",
            "AWRF (MinMaxRatio):  0.5797225914509614 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n",
            "AWRF (MaxMinRatio):  1.7249629646088922 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n",
            "AWRF (MaxAbsDiff):  1.5661430492857145 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n",
            "AWRF (MeanAbsDev):  1.566143049285714 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n",
            "AWRF (LTwo):  8.614723241859432 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n",
            "AWRF (Variance):  2.4528040508259545 avg_attention:  {'M': 7.45290142857143, 'W': 4.320615330000002}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exposure Rank Biased Precision Proportionality (ERBP)\n",
        "\n",
        "[ERBP](https://kcachel.github.io/FairRankTune/Metrics/#exposure-rank-biased-precision-proportionality-erbp) assesses if groups receive exposure proportional to their size whereby exposure is based on the Rank Biased Precision metric) This metric does not consider relevances or scores associate with items. Exposure in ERBE is determined differently compared to [exposure (EXP)](#group-exposure-exp). Specifically this calculation is based on the [Rank Biased Precision (RBP) metric](https://dl.acm.org/doi/10.1145/1416950.1416952).  The per-group metric is the group average exposure, whereby exposure is measured exactly as in [ERBE](https://kcachel.github.io/FairRankTune/Metrics/#exposure-rank-biased-precision-equality-erbe). Group average exposure for group $g_j$ in ranking $\\tau$ is $avgexpRBP(\\tau,g_j) = (1 - \\gamma) \\sum_{\\forall x \\in g_{j}}exposureRBP(\\tau,x_i)/|g_{j}|$.  The range of ERBP and its \"most fair\" value depends on the  `combo` variable.\n",
        "\n",
        "In the example below we calculate ERBP across all aggregation functions. We can see that the average exposures for 'M' (men) and 'W' (women) are always the same, but the ERBP value varies depenging on the meta-metric used.\n"
      ],
      "metadata": {
        "id": "PqBPwEvDPgsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df = pd.DataFrame([\"Joe\", \"Jack\", \"Nick\", \"David\", \"Mark\", \"Josh\", \"Dave\",\n",
        "                          \"Bella\", \"Heidi\", \"Amy\"])\n",
        "item_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\", Mark= \"M\", Josh= \"M\", Dave= \"M\", Jack= \"M\", Nick= \"M\")\n",
        "\n",
        "#Calculate ERBP\n",
        "decay = .75 #paramater representing gamma which controls the importance of higher ranks\n",
        "ERBPMaxMinDiff, ERBPs_MaxMinDiff = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'MaxMinDiff')\n",
        "print(\"ERBP (MaxMinDiff): \", ERBPMaxMinDiff, \"avg RBP exposure: \", ERBPs_MaxMinDiff)\n",
        "\n",
        "ERBPMinMaxRatio, ERBPs_MinMaxRatio = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'MinMaxRatio')\n",
        "print(\"ERBP (MinMaxRatio): \", ERBPMinMaxRatio, \"avg RBP exposure: \", ERBPs_MinMaxRatio)\n",
        "\n",
        "ERBPMaxMinRatio, ERBPs_MaxMinRatio = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'MaxMinRatio')\n",
        "print(\"ERBP (MaxMinRatio): \", ERBPMaxMinRatio, \"avg RBP exposure: \", ERBPs_MaxMinRatio)\n",
        "\n",
        "ERBPMaxAbsDiff, ERBPs_MaxAbsDiff = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'MaxAbsDiff')\n",
        "print(\"ERBP (MaxAbsDiff): \", ERBPMaxAbsDiff, \"avg RBP exposure: \", ERBPs_MaxAbsDiff)\n",
        "\n",
        "ERBPMeanAbsDev, ERBPs_MeanAbsDev = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'MeanAbsDev')\n",
        "print(\"ERBP (MeanAbsDev): \", ERBPMeanAbsDev, \"avg RBP exposure: \", ERBPs_MeanAbsDev)\n",
        "\n",
        "ERBPLTwo, ERBPs_LTwo = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'LTwo')\n",
        "print(\"ERBP (LTwo): \", ERBPLTwo, \"avg RBP exposure: \", ERBPs_LTwo)\n",
        "\n",
        "ERBPVariance, ERBPs_Variance = frt.Metrics.ERBP(ranking_df, item_group_dict, decay, 'Variance')\n",
        "print(\"ERBP (Variance): \", ERBPVariance, \"avg RBP exposure: \", ERBPs_Variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu425WvUPptz",
        "outputId": "6ecc1227-4f68-4c05-82e4-787324120c8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERBP (MaxMinDiff):  0.09806455884660993 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n",
            "ERBP (MinMaxRatio):  0.20780248467986195 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n",
            "ERBP (MaxMinRatio):  4.812261997447183 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n",
            "ERBP (MaxAbsDiff):  0.04903227942330497 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n",
            "ERBP (MeanAbsDev):  0.049032279423304966 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n",
            "ERBP (LTwo):  0.1264324689621714 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n",
            "ERBP (Variance):  0.0024041644254450554 avg RBP exposure:  {'M': 0.12378801618303571, 'W': 0.02572345733642578}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attribute Rank Parity (ARP)\n",
        "\n",
        "[ARP](https://kcachel.github.io/FairRankTune/Metrics/#attribute-rank-parity-arp) compares the number of mixed pairs won by groups in the ranking(s) and does not consider relevances or scores associate with items. It aligns with the fairness concept of statistical parity. ARP decomposes the ranking into pairwise comparisons, a mixed pair contains items from two different groups, the item \"on top\" is said to \"win\" the pair. The per-group metric is the average mixed pairs won by each group, calculated as $avgpairs(\\tau, g_i) = \\# ~mixedpairswon(g_i) / \\# totalmixedpairs(g_i)$ in ranking $\\tau$.  The range of ARP and its \"most fair\" value depends on the `combo` variable.\n",
        "\n",
        "In the example below we calculate ERBP across all aggregation functions. We can see that the average exposures for 'M' (men) and 'W' (women) are always the same, but the ERBP value varies depenging on the meta-metric used."
      ],
      "metadata": {
        "id": "1NOGYbI6-0dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df = pd.DataFrame([\"Joe\", \"Jack\", \"Nick\", \"David\", \"Mark\", \"Josh\",\n",
        "                          \"Bella\",  \"Dave\", \"Heidi\", \"Amy\"])\n",
        "item_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\", Mark= \"M\", Josh= \"M\", Dave= \"M\", Jack= \"M\", Nick= \"M\")\n",
        "\n",
        "#Calculate ARP\n",
        "ARPMaxMinDiff, group_pairs_MaxMinDiff = frt.Metrics.ARP(ranking_df, item_group_dict, 'MaxMinDiff')\n",
        "print(\"ARP (MaxMinDiff): \", ARPMaxMinDiff, \"group mixed pairs ratio: \", group_pairs_MaxMinDiff)\n",
        "\n",
        "ARPMinMaxRatio, group_pairs_MinMaxRatio = frt.Metrics.ARP(ranking_df, item_group_dict, 'MinMaxRatio')\n",
        "print(\"ARP (MinMaxRatio): \", ARPMinMaxRatio, \"group mixed pairs ratio: \", group_pairs_MinMaxRatio)\n",
        "\n",
        "ARPMaxMinRatio, group_pairs_MaxMinRatio = frt.Metrics.ARP(ranking_df, item_group_dict,'MaxMinRatio')\n",
        "print(\"ARP (MaxMinRatio): \", ARPMaxMinRatio, \"group mixed pairs ratio: \", group_pairs_MaxMinRatio)\n",
        "\n",
        "ARPMaxAbsDiff, group_pairs_MaxAbsDiff = frt.Metrics.ARP(ranking_df, item_group_dict,'MaxAbsDiff')\n",
        "print(\"ARP (MaxAbsDiff): \", ARPMaxAbsDiff, \"group mixed pairs ratio: \", group_pairs_MaxAbsDiff)\n",
        "\n",
        "ARPMeanAbsDev, group_pairs_MeanAbsDev = frt.Metrics.ARP(ranking_df, item_group_dict,'MeanAbsDev')\n",
        "print(\"ARP (MeanAbsDev): \", ARPMeanAbsDev, \"group mixed pairs ratio: \", group_pairs_MeanAbsDev)\n",
        "\n",
        "ARPLTwo, group_pairs_LTwo = frt.Metrics.ARP(ranking_df, item_group_dict,'LTwo')\n",
        "print(\"ARP (LTwo): \", ARPLTwo, \"group mixed pairs ratio: \", group_pairs_LTwo)\n",
        "\n",
        "ARPVariance, group_pairs_Variance = frt.Metrics.ARP(ranking_df, item_group_dict,'Variance')\n",
        "print(\"ARP (Variance): \", ARPVariance, \"group mixed pairs ratio: \", group_pairs_Variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaVTHUUp_KcX",
        "outputId": "0f475173-0423-4e03-88cb-76e1119e8406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARP (MaxMinDiff):  0.9047619047619047 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n",
            "ARP (MinMaxRatio):  0.05 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n",
            "ARP (MaxMinRatio):  20.0 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n",
            "ARP (MaxAbsDiff):  0.4523809523809524 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n",
            "ARP (MeanAbsDev):  0.45238095238095233 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n",
            "ARP (LTwo):  0.9535706854524183 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n",
            "ARP (Variance):  0.2046485260770975 group mixed pairs ratio:  {'M': 0.9523809523809523, 'W': 0.047619047619047616}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single Formulation Metrics Statistical Parity\n",
        "\n"
      ],
      "metadata": {
        "id": "bHcTa7JiCj36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalized Discounted KL-Divergence (NDKL)\n",
        "\n",
        "[NDKL](https://kcachel.github.io/FairRankTune/Metrics/#normalized-discounted-kl-divergence-ndkl) asseses the representation of groups in dsicrete prefixes of the ranking. It does not considers the relevance or scores associated with items. It aligns with the fairness cocnept of statistical parity and is assess on a single ranking. The NDKL of ranking $\\tau$ with respect to groups $G$ is defined as:\n",
        "$\\frac{1}{Z}\\sum^{|X|}_{i = 1}\\frac{1}{log_{2}(i +1 )}d_{KL}(D_{\\tau_i} || D_{X})$\n",
        "where $d_{KL}(D_{\\tau_i} || D_{X})$ is the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) score of the group proportions of the first $i$ positions in $\\tau$ and the group proportions of the item set $X$ and $Z = \\sum_{i = 1}^{| \\tau |} \\frac{1}{log_2(i + 1)}$. NDKL ranges from 0 to $\\infty$, and is most fair at 0."
      ],
      "metadata": {
        "id": "KIHRKyBcEGyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df = pd.DataFrame([\"Joe\", \"Jack\", \"Nick\", \"David\", \"Mark\", \"Josh\", \"Dave\",\n",
        "                          \"Bella\", \"Heidi\", \"Amy\"])\n",
        "item_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\", Mark= \"M\", Josh= \"M\", Dave= \"M\", Jack= \"M\", Nick= \"M\")\n",
        "\n",
        "NDKL= frt.Metrics.NDKL(ranking_df, item_group_dict)\n",
        "print(\"NDKL:\", NDKL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM5es_DdC1cA",
        "outputId": "475257dd-94de-4c80-f4da-28406c87dc2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDKL: 0.2925554332073208\n"
          ]
        }
      ]
    }
  ]
}