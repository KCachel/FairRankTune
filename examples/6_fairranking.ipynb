{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQhWC2LST25TqQdo1o/fm4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCachel/FairRankTune/blob/main/examples/6_fairranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "aPfbFCZkmYxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install [FairRankTune](https://https://github.com/KCachel/FairRankTune)."
      ],
      "metadata": {
        "id": "tfF1MsSEjhbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FairRankTune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYw3v4_jx74",
        "outputId": "2b59aaf5-5ecb-4770-b602-c686dca894e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: FairRankTune in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to import FairRankTune along with some other packages."
      ],
      "metadata": {
        "id": "UH63dmzEkAZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import FairRankTune as frt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from FairRankTune import RankTune, Metrics, Rankers"
      ],
      "metadata": {
        "id": "FyEGSwpWj9CZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fair Ranking Algorithms\n",
        "`FairRankTune` provides [fair ranking algorithms](https://kcachel.github.io/FairRankTune/Rankers/) in the `Rankers` module. These fair ranking algorithms can be used to re-rank a given ranking with the objective of making the resulting ranking fair.\n",
        "\n"
      ],
      "metadata": {
        "id": "-k6oQruomT4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epsilon-Greedy Re-Ranker\n",
        "Epsilon-Greedy takes as input a ranking and repeatedly swaps pairs of items so that each item has probability $\\epsilon$ (`epsilon`) of swapping with a random item below it. It does not require a specific notion of fairness or prior knowledge of group distributions. It does use random swapping, thus it is recommended to set a random seed for reproducability. To learn more see [Feng et al.](https://doi.org/10.1609/aaai.v36i11.21445) where it was introduced to improve group fairness.\n",
        "\n",
        "Note that `epsilon` must be between $[0,1]$ and a `seed` is passed for reproducability.\n",
        "\n",
        "Below we show Epsilon Greedy re-ranking a biased ranking produced by RankTune."
      ],
      "metadata": {
        "id": "GD3uDAiImd7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a biased (phi = 0) ranking of 1000 items, with two groups of 100 and 900 items each.\n",
        "seed = 2 #For reproducability\n",
        "group_proportions = np.asarray([.1, .9]) #Array of group proportions\n",
        "num_items = 1000 #1000 items to be in the generated ranking\n",
        "phi = 0 #Biased ranking\n",
        "r_cnt = 1 #Generate 1 ranking\n",
        "ranking_df, item_group_dict, scores_df = frt.RankTune.ScoredGenFromGroups(group_proportions,  num_items, phi, r_cnt, 'uniform', seed)\n",
        "\n",
        "#Calculate EXP with a MinMaxRatio\n",
        "EXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, item_group_dict, 'MinMaxRatio')\n",
        "print(\"EXP before Epsilon-Greedy: \", EXP_minmax, \"avg_exposures before Epsilon-Greedy: \", avg_exposures_minmax)\n",
        "\n",
        "\n",
        "#Rerank using Epsilon-Greedy\n",
        "\n",
        "seed = 2 #For reproducability\n",
        "epsilon = .6\n",
        "reranking_df, item_group_d, reranking_scores = frt.Rankers.EPSILONGREEDY(ranking_df, item_group_dict, scores_df, epsilon, seed)\n",
        "\n",
        "#Calculate EXP with a MinMaxRatio post Epsilon-Greedy\n",
        "EXP, avg_exposures= frt.Metrics.EXP(reranking_df, item_group_d, 'MinMaxRatio')\n",
        "print(\"EXP after Epsilon-Greedy: \", EXP, \"avg_exposures after Epsilon-Greedy: \", avg_exposures)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg85_2AumljP",
        "outputId": "31da31d9-addf-460b-a505-284d58da874d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXP before Epsilon-Greedy:  0.5420744267551784 avg_exposures before Epsilon-Greedy:  {0: 0.2093867087428094, 1: 0.11350318011191189}\n",
            "EXP after Epsilon-Greedy:  0.7689042373241246 avg_exposures after Epsilon-Greedy:  {0: 0.15541589156986096, 1: 0.1194999375755728}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the EXP score has increased (0.54 to 0.76) indicating the groups receive more comporable amounts of average exposure."
      ],
      "metadata": {
        "id": "s6pGkO6Hm3PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DetConstSort Re-Ranker\n",
        "\n",
        "DetConstSort takes as input a given ranking, and re-ranks items in it to create a top-k fair ranking. Fairness is achieved by setting the `distribution` dictionary.  In `distribution` the keys are group identifiers and the value is the desired group proportion. For any particular position k and for any group `g`, DetConstSort ensures that group occurs $\\lfloor$ `distribution[g]` $*k \\rfloor$ in the resulting ranking. DetConstSort algorithm also tries improve the utility of the ranking by ensuring that items with better scores are placed higher in the ranking so long as the ranking satisfies the feasibility criteria. To learn more see [Geyik et al.](https://dl.acm.org/doi/10.1145/3292500.3330691).\n",
        "\n",
        "Below we show DetConstSort re-ranking the same biased ranking as above from RankTune."
      ],
      "metadata": {
        "id": "yaLQk9LnnDix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a biased (phi = 0) ranking of 1000 items, with two groups of 100 and 900 items each.\n",
        "seed = 2 #For reproducability\n",
        "group_proportions = np.asarray([.1, .9]) #Array of group proportions\n",
        "num_items = 1000 #1000 items to be in the generated ranking\n",
        "phi = 0 #Biased ranking\n",
        "r_cnt = 1 #Generate 1 ranking\n",
        "ranking_df, item_group_dict, scores_df = frt.RankTune.ScoredGenFromGroups(group_proportions,  num_items, phi, r_cnt, 'uniform', seed)\n",
        "\n",
        "#Calculate EXP with a MinMaxRatio\n",
        "EXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, item_group_dict, 'MinMaxRatio')\n",
        "print(\"EXP before DetConstSort: \", EXP_minmax, \"avg_exposures before DetConstSort: \", avg_exposures_minmax)\n",
        "\n",
        "#Rerank using DetConstSort\n",
        "distribution = dict(zip([0, 1], [.1, .9])) #set distribution for statistical parity\n",
        "k = 800 #only ranking 800 items of the provided 1000\n",
        "reranking_df, item_group_d, reranking_scores = frt.Rankers.DETCONSTSORT(ranking_df, item_group_dict, scores_df, distribution, k)\n",
        "\n",
        "#Calculate EXP with a MinMaxRatio post DetConstSort\n",
        "EXP, avg_exposures= frt.Metrics.EXP(reranking_df, item_group_d, 'MinMaxRatio')\n",
        "print(\"EXP after DetConstSort: \", EXP, \"avg_exposures after DetConstSort: \", avg_exposures)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySa7xfNMoMkA",
        "outputId": "abf14371-6b60-49c0-f9d1-e8f086abeed7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXP before DetConstSort:  0.5420744267551784 avg_exposures before DetConstSort:  {0: 0.2093867087428094, 1: 0.11350318011191189}\n",
            "EXP after DetConstSort:  0.8892953016490737 avg_exposures after DetConstSort:  {0: 0.14259348136033306, 1: 0.12680771301952895}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the EXP score has increased (0.54 to 0.88) indicating the groups receive more comporable amounts of average exposure in this new top-800 ranking."
      ],
      "metadata": {
        "id": "YRoBYEQiqASc"
      }
    }
  ]
}