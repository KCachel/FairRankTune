{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"\ud83d\udccd Introduction","text":"<p>FairRankTune is a  an open-source Python toolkit supporting end-to-end fair ranking workflows, analysis, auditing, and experimentation. FairRankTune provides researchers, practitioners, and educators with a self-contained module for generating ranked data, ranking strategies, and popular ranking-based fairness metrics.</p> <p>For a quick overview, follow the Usage section.</p> <p>For a in-depth overview, follow the Examples section.</p>"},{"location":"#features","title":"\u2728 Features","text":""},{"location":"#fairness-aware-ranked-data-generation","title":"\ud83c\udfa8 Fairness-Aware Ranked Data Generation","text":"<p><code>RankTune</code> is a pseudo-stochastic data generation method for creating fairness-aware ranked lists using the fairness concept of statistical parity. Included in the <code>RankTune</code> module, it creates ranking(s) based on the <code>phi</code> representativeness parameter. When <code>phi = 0</code> then the generated ranked list(s) does not represent groups fairly, and as <code>phi</code> increases groups are represented more and more fairly; thus <code>phi = 1</code> groups are fairly represented. RankTune uses a pseudo-random process to generate fairness-aware ranked data. RankTune can generate ranked data from user provided group sizes, from existing datasets, along with producing relevance scores accompanying the ranked list(s). </p> <p>Please refer to the documentation for additional information. </p>"},{"location":"#metrics","title":"\ud83d\udccf Metrics","text":"<p><code>FairRankTune</code> provides several metrics for evaluating the fairness of ranked lists in the <code>Metrics</code> module. The table below provides a high-level overview of each metric. These metrics encompass a variety of fair ranking metrics, including both group and individual fairness, along with both score-based and statistical parity metrics. </p> Metric Abbreviation Fairness (Group or Individual) Score-based Statistical Parity Reference Group Exposure EXP Group No Yes Singh et al. Exposure Utility EXPU Group Yes No Singh et al. Exposure Realized Utility EXPRU Group Yes No Singh et al. Attention Weighted Rank Fairness AWRF Group No Yes Sapiezynski et al. Exposure Rank Biased Precision Equality ERBE Group No No Kirnap et al. Exposure Rank Biased Precision Proportionality ERBP Group No Yes Kirnap et al. Exposure Rank Biased Precision Proportional to Relevance ERBR Group Yes No Kirnap et al. Attribute Rank Parity ARP Group No Yes Cachel et al. Normalized Discounted KL-Divergence NDKL Group No Yes Geyik et al. Inequity of Amortized Attention IAA Individual Yes No Biega et al. <p>Please refer to the Metrics documentation for further details. </p>"},{"location":"#fair-ranking-methods","title":"\u2696\ufe0f Fair Ranking Methods","text":"<p><code>FairRankTune</code> provides several fair ranking algorithms in the <code>Rankers</code> module. The DetConstSort and Epsilon-Greedy fair ranking algorithms can be used to re-rank a given ranking with the objective of making the resulting ranking fair. </p> <p>Please refer to the documentation for further details. </p>"},{"location":"#requirements","title":"\ud83d\udd0c Requirements","text":"<p><pre><code>python&gt;=3.8\n</code></pre> As of <code>v.0.0.6</code>, FairRankTune requires <code>python&gt;=3.8</code>.</p>"},{"location":"#installation","title":"\ud83d\udcbe Installation","text":"<pre><code>pip install FairRankTune\n</code></pre>"},{"location":"#usage","title":"\ud83d\udca1 Usage","text":""},{"location":"#fairness-aware-ranked-data-generation_1","title":"\ud83c\udfa8 Fairness-Aware Ranked Data Generation","text":"<p><code>RankTune</code> can be used to generate ranking(s) from <code>group_proportions</code>, a numpy array with each group's proportion of the total items, <code>num_items</code>, by using the <code>GenFromGroups()</code> function.</p> GenFromGroups() function<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0.1) ranking of 1000 items, with four groups of 100,\n# 200, 300, and 400 items each.\ngroup_proportions = np.asarray([.1, .2, .3, .4]) #Array of group proportions\nnum_items = 1000 #1000 items to be in the generated ranking\nphi = 0.1\nr_cnt = 1 #Generate 1 ranking\nseed = 10 #For reproducibility\nranking_df, item_group_dict = frt.RankTune.GenFromGroups(group_proportions, \n  num_items,phi, r_cnt, seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, item_group_dict,\n  'MinMaxRatio')\nprint(\"EXP of generated ranking: \", EXP_minmax, \"avg_exposures: \",\n  avg_exposures_minmax)\n</code></pre> <p>Output: <pre><code>EXP of generated ranking:  0.511665941043515\n avg_exposures:  {0: 0.20498798214669187, 1: 0.13126425437156242, 2: 0.11461912123646827, 3: 0.10488536878769836}\n</code></pre> Can confirm this is an unfair ranking by the low EXP value.</p> <p><code>RankTune</code> can be used to generate ranking(s) from <code>item_group_dict</code>, a dictionary of items where the keys are each item's group by using the <code>GenFromItems()</code> function.</p> GenFromItems() function<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0.1) ranking\nitem_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\",\n  Amy = \"W\", Jill= \"W\", Jane= \"W\", Dave= \"M\", Nancy= \"W\", Nick= \"M\")\nphi = 0.1\nr_cnt = 1 #Generate 1 ranking\nseed = 10 #For reproducibility\nranking_df, item_group_dict = frt.RankTune.GenFromItems(item_group_dict, phi,\n  r_cnt, seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, \n  item_group_dict, 'MinMaxRatio')\nprint(\"EXP of generated ranking: \", EXP_minmax, \n  \"avg_exposures: \", avg_exposures_minmax)\n</code></pre> <p>Output: <pre><code>EXP of generated ranking:  0.5158099476966725 avg_exposures:  {'M': 0.6404015779112127, 'W': 0.33032550440724917}\n</code></pre> We can confirm this is a biased ranking base don the low EXP score and large difference in average exposure between the 'M' and 'W' groups.</p> <p>For further detail on how to use <code>RankTune</code> to generate relevance scores see the RankTune documentation.</p>"},{"location":"#metrics_1","title":"\ud83d\udccf Metrics","text":"<p>The <code>Metric</code> library can be used to assess the fairness of rankings. Our current offering contains 10 core metrics across individual and group fairness.  Group fairness metrics include both metrics that incorporate relevance scores associated with items, so-called score-based fairness, and statistical parity metrics that are based on the representation of groups. A key functionality of the  <code>Metrics</code> library in <code>FairRankTune</code>  is providing toolkit users multiple choices for how to calculate a given top-level fairness metric. For instance, for group exposure,  a popular fairness criteria,  <code>Metrics</code> offers seven ways of calculating a top-level exposure metric (e.g., min-max ratios, max absolute difference, L-2 norms of per-group exposures, etc.).</p> <p>Calculate Group Exposure EXP with MaxMinDiff<pre><code>import FairRankTune as frt\nimport pandas as pd\nimport numpy as np\nranking_df = pd.DataFrame([\"Joe\", \"Jack\", \"Nick\", \"David\",\n  \"Mark\", \"Josh\", \"Dave\", \"Bella\", \"Heidi\", \"Amy\"])\nitem_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\",\n  Heidi= \"W\", Amy = \"W\", Mark= \"M\", Josh= \"M\", Dave= \"M\", Jack= \"M\", Nick= \"M\")\n#Calculate EXP with a MaxMinDiff\nEXP, avg_exposures = frt.Metrics.EXP(ranking_df, item_group_dict,\n  'MaxMinDiff')\nprint(\"EXP: \", EXP, \"avg_exposures: \", avg_exposures)\n</code></pre> Output: <pre><code>&gt;&gt;&gt; EXP:  0.21786100126614577 avg_exposures:  {'M': 0.5197142341886783, 'W': 0.3018532329225326}\n</code></pre></p>"},{"location":"#fair-ranking-algorithms","title":"\u2696\ufe0f Fair Ranking Algorithms","text":"Epsilon-Greedy Algorithm<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\nimport random\n\n#Generate a biased (phi = 0) ranking of 1000 items, with two groups of \n#100 and 900 items each.\ngroup_proportions = np.asarray([.1, .9]) #Array of group proportions\nnum_items = 1000 #1000 items to be in the generated ranking\nphi = 0 #Biased ranking\nr_cnt = 1 #Generate 1 ranking\nranking_df, item_group_dict, scores_df = frt.RankTune.ScoredGenFromGroups(\n  group_proportions, num_items, phi, r_cnt, 'uniform', seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, \n  item_group_dict, 'MinMaxRatio')\nprint(\"EXP before Epsilon-Greedy: \", EXP_minmax,\n  \"avg_exposures before Epsilon-Greedy: \", avg_exposures_minmax)\n\n\n#Rerank using Epsilon-Greedy\nseed = 2 #For reproducibility\nepsilon = .6 \nreranking_df, item_group_d, reranking_scores = frt.Rankers.EPSILONGREEDY(\n  ranking_df, item_group_dict, scores_df, epsilon, seed)\n\n#Calculate EXP with a MinMaxRatio post Epsilon-Greedy\nEXP, avg_exposures= frt.Metrics.EXP(reranking_df, item_group_d,\n  'MinMaxRatio')\nprint(\"EXP after Epsilon-Greedy: \", EXP,\n  \"avg_exposures after Epsilon-Greedy: \", avg_exposures)\n</code></pre> <p>Output: <pre><code>EXP before Epsilon-Greedy:  0.5420744267551784 avg_exposures before Epsilon-Greedy:  {0: 0.2093867087428094, 1: 0.11350318011191189}\nEXP after Epsilon-Greedy:  0.7689042373241246 avg_exposures after Epsilon-Greedy:  {0: 0.15541589156986096, 1: 0.1194999375755728}\n</code></pre> We can see that the EXP fairness score improved from running Epsilon-Greedy. For more usage examples please see the documentation.</p>"},{"location":"#examples","title":"\ud83d\udcd6 Examples","text":"Topic Link Quickstart RankTune Overview RankTune Augmenting Datasets Statistical Parity Metrics Score-based (Group &amp; Individual) Metrics Using Fair Ranking Algorithms"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Check out the documentation for more details and example notebooks.</p>"},{"location":"#citation","title":"\ud83c\udf93 Citation","text":"<p>If you end up using FairRankTune in your work, please consider citing it:</p> BibTeX <pre><code>@misc{CachelFRT,\n  author    = {Kathleen Cachel},\n  title     = {FairRankTune: A Python Library for Fair Ranking},\n  year = {2023},\n  publisher = {GitHub},\n  howpublished = {\\url{https://github.com/KCachel/fairranktune}}\n}\n</code></pre>"},{"location":"#feature-requests","title":"\u2049\ufe0f Feature Requests","text":"<p>We believe in open-source community driven software. Would you like to see other functionality implemented? Please, open a feature request. Is there a bug or issue ? Please, open a github issue.</p>"},{"location":"#want-to-contribute","title":"\ud83d\udc4b Want to contribute?","text":"<p>Would you like to contribute? Please, send me an e-mail.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>FairRankTune is open-sourced software licensed under the BSD-3-Clause license.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#get-started-fast","title":"Get Started Fast","text":"<p>This notebook shows you all the functionality of FairRankTune. </p> <p></p>"},{"location":"examples/#how-to-use-ranktune-for-data-generation","title":"How to use RankTune for Data Generation","text":"<p>This notebooks shows you the functionality of the RankTune data generator method, and how you can use its different features.</p> <p></p>"},{"location":"examples/#how-to-use-ranktune-to-create-partially-synthetic-data","title":"How to use RankTune to Create Partially Synthetic Data","text":"<p>This notebook shows you how to combine the data generation capabilities of RankTune with an existing dataset to create partially synthetic data.</p> <p></p>"},{"location":"examples/#how-to-measure-statistical-parity-group-fairness-in-rankings","title":"How to Measure Statistical Parity Group Fairness in Rankings","text":"<p>This notebook shows you how to use FairRankTune's Metric library to audit rankings for statistical parity.</p> <p> </p>"},{"location":"examples/#how-to-measure-score-based-group-and-individual-fairness-in-rankings","title":"How to Measure Score-based Group and Individual Fairness in Rankings","text":"<p>This notebook shows you how to use FairRankTune's Metric library to audit rankings in terms of score-based group and individual fairness.</p> <p> </p>"},{"location":"examples/#how-to-use-the-fair-ranking-algorithms","title":"How to use the Fair Ranking Algorithms","text":"<p>This notebook shows you how to use FairRankTune's Rankers module to adjust rankings to be fair using existing algorithms from the fairness literature.</p> <p></p>"},{"location":"examples/#sample-demonstration","title":"Sample Demonstration","text":"<p>This provides an end-to-end sample demonstration of the FairRankTune toolkit.</p> <p></p>"},{"location":"metrics/","title":"Metrics","text":""},{"location":"metrics/#overview","title":"Overview","text":"<p><code>FairRankTune</code> provides several metrics for evaluating the fairness of ranked lists in the <code>Metrics</code> module. The table below provides a high-level overview of each metric. These metrics encompass a variety of fair ranking metrics, including both group and individual fairness, along with both score-based and statistical parity metrics. </p> <p>What is group, individual, score-based, and or statistical parity fairness? </p> <p>Group Fairness: Measures if groups of items are being treated similarly. For example, we might want to know if groups are making it to the top of ranking(s).</p> <p>Individual Fairness: Measures if similar items are being treated similarly. For example, we might want to know if items that are similar are ranked in similar positions in rankings.</p> <p>Score-based Fairness: Measures if exposure (or attention, clicks, views etc.) are proportional to item relevance or group relevance. For example, in the form of individual fairness, we might want to know if items that are similar received similar amounts of exposure in rankings. Or for group fairness, we might want to know if groups are click-on proportional to their relevance.</p> <p>Statistical Parity Fairness: A sub-type of Group Fairness, measures if groups receive a proportional share of the positive outcome; in ranking(s) the positive outcome can be the exposure or attention of the viewer or a share of top-ranked position. Statistical Parity is also known as Demographic Parity and explicitly does not use relevance scores. For example, we might want to know if groups receive comparable amounts of exposure.</p> Metric Abbreviation Fairness (Group or Individual) Score-based Statistical Parity Reference Group Exposure EXP Group No Yes Singh et al. Exposure Utility EXPU Group Yes No Singh et al. Exposure Realized Utility EXPRU Group Yes No Singh et al. Attention Weighted Rank Fairness AWRF Group No Yes Sapiezynski et al. Exposure Rank Biased Precision Equality ERBE Group No No Kirnap et al. Exposure Rank Biased Precision Proportionality ERBP Group No Yes Kirnap et al. Exposure Rank Biased Precision Proportional to Relevance ERBR Group Yes No Kirnap et al. Attribute Rank Parity ARP Group No Yes Cachel et al. Normalized Discounted KL-Divergence NDKL Group No Yes Geyik et al. Inequity of Amortized Attention IAA Individual Yes No Biega et al."},{"location":"metrics/#modular-metric-implementation","title":"Modular Metric Implementation","text":"<p>A key functionality of the  <code>Metrics</code> library in <code>FairRankTune</code>  is providing toolkit users multiple choices for how to calculate a given top-level fairness metric. For instance, for group exposure,  a popular fairness criteria,  <code>Metrics</code> offers seven ways of calculating a top-level exposure metric (e.g., min-max ratios, max absolute difference, L-2 norms of per-group exposures, etc.).</p> <p>Below are the formulas supported for combining per-group style metrics. In the formulas \\(V = [V_{1}, ..., V_{g}\\)] is an array of per-group metrics and \\(G\\) is the number of groups. The <code>combo</code> variable is used directly in the function call. Depending on the formula used for aggregating per-group metrics the range of the given fairness metric varies. The range and its corresponding \"most fair\" value is provided in the table. </p> Combo Variable in <code>FairRankTune</code> Formula Range Most Fair <code>MinMaxRatio</code> \\(min_{g} V / max_{g} V\\) [0,1] 1 <code>MaxMinRatio</code> \\(max_{g} V / min_{g} V\\) [1, \\(\\infty\\)] 1 <code>MaxMinDiff</code> \\(max_{g} V - min_{g} V\\) [0,1] 0 <code>MaxAbsDiff</code> \\(max_{g} \\mid V - V_{mean} \\mid\\) [0, \\(\\infty\\)] 0 <code>MeanAbsDev</code> \\(\\frac{1}{G} \\sum_{g} \\mid V - V_{mean}\\mid\\) [0, \\(\\infty\\)] 0 <code>LTwo</code> \\(\\lVert V \\rVert_2^2\\) [0, \\(\\infty\\)] 0 <code>Variance</code> \\(\\frac{1}{G - 1} \\sum_{g} (V_{g} - V_{mean})^2\\) [0, \\(\\infty\\)] 0 <p>Usage:</p> <p><pre><code>from FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0) ranking of 1000 items, with two groups of 100\n#and 900 items each. \nranking_df, item_group_dict = frt.RankTune.GenFromGroups(np.asarray([.1, .9]),\n  1000, 0, 1)\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df,\n  item_group_dict, 'MinMaxRatio')\nprint(\"EXP_minmax: \", EXP_minmax, \"avg_exposures: \", avg_exposures)\n#Calculate EXP with a MaxAbsDiff\nEXP_maxabs, avg_exposures_maxabs = frt.Metrics.EXP(ranking_df,\n  item_group_dict, 'MaxAbsDiff')\nprint(\"EXP_maxabs: \", EXP_maxabs, \"avg_exposures: \", avg_exposures)\n</code></pre> Outputs: <pre><code>EXP_minmax:  0.5420744267551784 avg_exposures:  {0: 0.2093867087428094, 1: 0.11350318011191189}\nEXP_maxabs:  0.04794176431544876 avg_exposures:  {0: 0.2093867087428094, 1: 0.11350318011191189}\n</code></pre> Notice that the <code>EXP_minmax</code> and <code>EXP_maxabs</code> are different. The second returned object is a dictionary with groups as keys and the values represent the per-group metric. In this example that is the average exposure of group 0 and group 1.</p> <p>The following metrics have meta-metric functionality: EXP, EXPU, EXRU, AWRF, ERBE, ERBP, ERBPR, and ARP. To specify the desired aggregation form pass any of the strings in the table above as the  <code>combo</code> input variable. </p>"},{"location":"metrics/#supported-fair-ranking-metrics","title":"Supported Fair Ranking Metrics","text":"<p>All metric functions take as the inputted <code>ranking_df</code> parameter a pandas dataframe of the ranking(s) to be evaluated. These rankings need not have the same number of items, and items can be represented as floats, ints, or strings.</p> <p>All group fairness metric functions take as the inputted <code>item_group_dict</code> parameter a python dictionary of items and their group membership. Items are keys, and the value represents the group of that item (ints or strings are equally fine). Note, that all group metrics supported in <code>FairRankTune</code> support multiple groups.</p>"},{"location":"metrics/#group-exposure-exp","title":"Group Exposure (EXP)","text":"<p>EXP compares the average exposures of groups in the ranking(s) and does not consider relevances or scores associate with items. It aligns with the fairness concept of statistical parity. The per-group metric is the group average exposure, whereby the exposure of item \\(x_i\\) in ranking \\(\\tau\\) is \\(exposure(\\tau,x_i) = 1 / log_2(\\tau(x_i)+1))\\) and the average exposure for group \\(g_j\\) is \\(avgexp(\\tau,g_j) = \\sum_{\\forall x \\in g_{j}}exposure(\\tau,x_i)/|g_{j}|\\). The range of EXP and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Usage:</p> <p><pre><code>#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures = frt.Metrics.EXP(ranking_df,\n  item_group_dict, 'MinMaxRatio')\n</code></pre> The first returned object is float specifying the EXP value and the second returned object is a dictionary of average exposures for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3219819.3220088,\nauthor = {Singh, Ashudeep and Joachims, Thorsten},\ntitle = {Fairness of Exposure in Rankings},\nyear = {2018},\nisbn = {9781450355520},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3219819.3220088},\ndoi = {10.1145/3219819.3220088},\nbooktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\&amp; Data Mining},\npages = {2219\u20132228},\nnumpages = {10},\nkeywords = {algorithmic bias, fairness, position bias, equal opportunity, fairness in rankings},\nlocation = {London, United Kingdom},\nseries = {KDD '18}\n}\n</code></pre> <p>Fairness of Group Exposure is also introduced in Diaz et al..</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3340531.3411962,\nauthor = {Diaz, Fernando and Mitra, Bhaskar and Ekstrand, Michael D. and Biega, Asia J. and Carterette, Ben},\ntitle = {Evaluating Stochastic Rankings with Expected Exposure},\nyear = {2020},\nisbn = {9781450368599},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3340531.3411962},\ndoi = {10.1145/3340531.3411962},\nbooktitle = {Proceedings of the 29th ACM International Conference on Information \\&amp; Knowledge Management},\npages = {275\u2013284},\nnumpages = {10},\nkeywords = {fairness, evaluation, diversity, learning to rank},\nlocation = {Virtual Event, Ireland},\nseries = {CIKM '20}\n}\n</code></pre>"},{"location":"metrics/#exposure-utility-expu","title":"Exposure Utility (EXPU)","text":"<p>EXPU assesses if groups receive exposure proportional to their relevance in the ranking(s). This is a form of group fairness that considers the scores (relevances) associated with items. The per-group metric is the ratio of group average exposure and group average utility, whereby group average exposure is measured exactly as in EXP. Group average utility for group \\(g_j\\) is \\(avgutil(\\tau,g_j) = \\sum_{\\forall x \\in g_{j}}x_i^{util_{\\tau}}/|g_{j}|\\), where \\(x_i^{util_{\\tau}}\\) is the utility (or relevance score) for candidate \\(x_i\\) in ranking \\(\\tau\\).  The range of EXPU and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Singh et al. refer to EXPU as \"Disparate Treatment\", as pointed out by Raj et al. this terminology, is inconsistent with the use of these terms in the broader algorithmic fairness literature, thus <code>FairRankTune</code> uses the term \"Exposure Utility\" a introduced in [Raj et al.}(https://dl.acm.org/doi/10.1145/3477495.3532018).</p> <p>Usage:</p> <p><pre><code>#Calculate EXPU with a MinMaxRatio\nEXPU, per_group = frt.Metrics.EXPU(ranking_df, item_group_dict,\n  relevance_df, 'MinMaxRatio')\n</code></pre> Note that the relevance scores associated with the ranking(s) in <code>relevance_df</code> must be between 0 and 1. The first returned object is a float specifying the EXPU value and the second returned object is a dictionary of average exposure and average utility ratios for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3219819.3220088,\nauthor = {Singh, Ashudeep and Joachims, Thorsten},\ntitle = {Fairness of Exposure in Rankings},\nyear = {2018},\nisbn = {9781450355520},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3219819.3220088},\ndoi = {10.1145/3219819.3220088},\nbooktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\&amp; Data Mining},\npages = {2219\u20132228},\nnumpages = {10},\nkeywords = {algorithmic bias, fairness, position bias, equal opportunity, fairness in rankings},\nlocation = {London, United Kingdom},\nseries = {KDD '18}\n}\n</code></pre>"},{"location":"metrics/#exposure-realized-utility-expru","title":"Exposure Realized Utility (EXPRU)","text":"<p>EXPRU assesses if groups are click-on proportional to their relevance in the ranking(s). This is a form of group fairness that considers the scores (relevances) associated with items. The per-group metric is the ratio of group average click-through rate and group average utility, whereby  group average utility is measured exactly as in EXPU. The average click-through rate for group \\(g_j\\) in \\(\\tau\\) is \\(avgctr(\\tau,g_j) = \\sum_{\\forall x \\in g_{j}}x_i^{ctr_{\\tau}}/|g_{j}|\\), where \\(x_i^{ctr_{\\tau}}\\) is the click-through rate for candidate \\(x_i\\) in ranking \\(\\tau\\).  The range of EXPRU and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Singh et al. refer to EXPRU as \"Disparate Impact\", as pointed out by Raj et al. this terminology, is inconsistent with the use of these terms in the broader algorithmic fairness literature, thus <code>FairRankTune</code> uses the term \"Exposure Realized Utility\" a introduced in Raj et al..</p> <p>Usage: <pre><code>#Calculate EXPRU with a MinMaxRatio\nEXPRU, per_group = frt.Metrics.EXPRU(ranking_df, item_group_dict,\n  relevance_df, ctr_df,'MinMaxRatio')\n</code></pre> Note that the relevance scores associated with the ranking(s) in <code>relevance_df</code> must be between 0 and 1 and the click-through-rates in <code>ctr_df</code> must be between 0 (no clicks) or 1 (100% ctr). The first returned object is a float specifying the EXPRU value and the second returned object is a dictionary of  average utility and average click-through rate ratios for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3219819.3220088,\nauthor = {Singh, Ashudeep and Joachims, Thorsten},\ntitle = {Fairness of Exposure in Rankings},\nyear = {2018},\nisbn = {9781450355520},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3219819.3220088},\ndoi = {10.1145/3219819.3220088},\nbooktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\&amp; Data Mining},\npages = {2219\u20132228},\nnumpages = {10},\nkeywords = {algorithmic bias, fairness, position bias, equal opportunity, fairness in rankings},\nlocation = {London, United Kingdom},\nseries = {KDD '18}\n}\n</code></pre>"},{"location":"metrics/#attention-weighted-rank-fairness-awrf","title":"Attention Weighted Rank Fairness (AWRF)","text":"<p>AWRF compares the average attention of groups in the ranking(s)and does not consider relevances or scores associate with items. It aligns with the fairness concept of statistical parity.  Attention compared to exposure uses a geometric discount on the \"attention\" assigned to positions in a ranking. The per-group metric is the group average attention, whereby the attention score for item \\(x_i\\) in ranking \\(\\tau\\) as \\(attention(\\tau,x_i) = 100 \\times (1 - p) ^{(\\tau(x_i) -1)} \\times p\\), where \\(p\\) is a parameter representing the proportion of attention received by the first (top) ranked item.  The range of AWRF and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Usage: <pre><code>#Calculate AWRF with a MinMaxRatio\np = .01 #parameter representing the proportion of attention...\n# received by the first position\nAWRF, per_group = frt.Metrics.AWRF(ranking_df, item_group_dict, p, 'MinMaxRatio')\n</code></pre> The first returned object is a float specifying the AWRF value and the second returned object is a dictionary of average attention values for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3308560.3317595,\nauthor = {Sapiezynski, Piotr and Zeng, Wesley and E Robertson, Ronald and Mislove, Alan and Wilson, Christo},\ntitle = {Quantifying the Impact of User Attention on Fair Group Representation in Ranked Lists},\nyear = {2019},\nisbn = {9781450366755},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3308560.3317595},\ndoi = {10.1145/3308560.3317595},\npages = {553\u2013562},\nnumpages = {10},\nkeywords = {group fairness, ranked lists, information retrieval},\nlocation = {San Francisco, USA},\nseries = {WWW '19}\n}\n</code></pre>"},{"location":"metrics/#exposure-rank-biased-precision-equality-erbe","title":"Exposure Rank Biased Precision Equality (ERBE)","text":"<p>ERBE assesses if groups receive equal exposure whereby exposure is based on the Rank Biased Precision metric. This metric does not consider relevances or scores associate with items. Exposure in ERBE is determined differently compared to exposure (EXP). Specifically this calculation is based on the Rank Biased Precision (RBP) metric. The per-group metric is the group RBP-based exposure, whereby the Rank Biased Precision exposure of item \\(x_i\\) in ranking \\(\\tau\\) is \\(exposureRBP(\\tau,x_i) = \\gamma^{(1-\\tau(x_i))}\\) and the exposure for group \\(g_j\\) is \\(expRBP(\\tau,g_j) = (1 - \\gamma) \\sum_{\\forall x \\in g_{j}}exposureRBP(\\tau,x_i)\\).  The range of ERBE and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Usage: <pre><code>#Calculate ERBE with a MinMaxRatio\ndecay = .01 #parameter representing gamma which...\n#controls the importance of higher ranks\nERBE, per_group = frt.Metrics.ERBE(ranking_df, item_group_dict,\n  decay, 'MinMaxRatio')\n</code></pre> The first returned object is a float specifying the ERBE value and the second returned object is a dictionary of total exposure values for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3442381.3450080,\nauthor = {K\\i{}rnap, \\\"{O}mer and Diaz, Fernando and Biega, Asia and Ekstrand, Michael and Carterette, Ben and Yilmaz, Emine},\ntitle = {Estimation of Fair Ranking Metrics with Incomplete Judgments},\nyear = {2021},\nisbn = {9781450383127},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3442381.3450080},\ndoi = {10.1145/3442381.3450080},\nbooktitle = {Proceedings of the Web Conference 2021},\npages = {1065\u20131075},\nnumpages = {11},\nkeywords = {fair ranking, evaluation, information retrieval, fairness},\nlocation = {Ljubljana, Slovenia},\nseries = {WWW '21}\n}\n</code></pre>"},{"location":"metrics/#exposure-rank-biased-precision-proportionality-erbp","title":"Exposure Rank Biased Precision Proportionality (ERBP)","text":"<p>ERBP assesses if groups receive exposure proportional to their size whereby exposure is based on the Rank Biased Precision metric) This metric does not consider relevances or scores associate with items. Exposure in ERBE is determined differently compared to exposure (EXP). Specifically this calculation is based on the Rank Biased Precision (RBP) metric.  The per-group metric is the group average exposure, whereby exposure is measured exactly as in ERBE. Group average exposure for group \\(g_j\\) is \\(avgexpRBP(\\tau,g_j) = (1 - \\gamma) \\sum_{\\forall x \\in g_{j}}exposureRBP(\\tau,x_i)/|g_{j}|\\).  The range of ERBP and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Usage: <pre><code>#Calculate ERBP with a MinMaxRatio\ndecay = .01 #parameter representing gamma which...\n#controls the importance of higher ranks\nERBP, per_group = frt.Metrics.ERBP(ranking_df, item_group_dict,\n  decay, 'MinMaxRatio')\n</code></pre> The first returned object is a float specifying the ERBP value and the second returned object is a dictionary of average exposure values for each group (keys are group ids). Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3442381.3450080,\nauthor = {K\\i{}rnap, \\\"{O}mer and Diaz, Fernando and Biega, Asia and Ekstrand, Michael and Carterette, Ben and Yilmaz, Emine},\ntitle = {Estimation of Fair Ranking Metrics with Incomplete Judgments},\nyear = {2021},\nisbn = {9781450383127},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3442381.3450080},\ndoi = {10.1145/3442381.3450080},\nbooktitle = {Proceedings of the Web Conference 2021},\npages = {1065\u20131075},\nnumpages = {11},\nkeywords = {fair ranking, evaluation, information retrieval, fairness},\nlocation = {Ljubljana, Slovenia},\nseries = {WWW '21}\n}\n</code></pre>"},{"location":"metrics/#exposure-rank-biased-precision-proportional-to-relevance-erbr","title":"Exposure Rank Biased Precision Proportional to Relevance (ERBR)","text":"<p>ERBR assesses if groups receive exposure proportional to how many relevant items are in the group. It aligns with the fairness concept of statistical parity. This is a form of group fairness that considers the scores (relevances) associated with items. The per-group metric is the ratio of group exposure and the number of items belonging to the given group that are relevant, whereby exposure is measured exactly as in ERBE. This ratio for group \\(g_j\\) is \\(expRBP2rel(\\tau,g_j) = (1 - \\gamma) \\sum_{\\forall x \\in g_{j}}exposureRBP(\\tau,x_i)/|g_{j}^{rel}|\\), where \\(|g_{j}^{rel}|\\) is the count of relevant items in group \\(g_{j}\\).  The range of ERBR and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Usage: <pre><code>#Calculate ERBR with a MinMaxRatio\ndecay = .01 #parameter representing gamma which...\n#controls the importance of higher ranks\nERBR, per_group = frt.Metrics.ERBP(ranking_df, item_group_dict,\n  relevance_df, decay, 'MinMaxRatio')\n</code></pre> Note that the relevance scores associated with the ranking(s) in <code>relevance_df</code> must be either 0 or 1. The first returned object is float specifying the ERBR value and the second returned object is a dictionary of exposure and relevance ratios for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3442381.3450080,\nauthor = {K\\i{}rnap, \\\"{O}mer and Diaz, Fernando and Biega, Asia and Ekstrand, Michael and Carterette, Ben and Yilmaz, Emine},\ntitle = {Estimation of Fair Ranking Metrics with Incomplete Judgments},\nyear = {2021},\nisbn = {9781450383127},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3442381.3450080},\ndoi = {10.1145/3442381.3450080},\nbooktitle = {Proceedings of the Web Conference 2021},\npages = {1065\u20131075},\nnumpages = {11},\nkeywords = {fair ranking, evaluation, information retrieval, fairness},\nlocation = {Ljubljana, Slovenia},\nseries = {WWW '21}\n}\n</code></pre>"},{"location":"metrics/#attribute-rank-parity-arp","title":"Attribute Rank Parity (ARP)","text":"<p>ARP compares the number of mixed pairs won by groups in the ranking(s) and does not consider relevances or scores associate with items. It aligns with the fairness concept of statistical parity. ARP decomposes the ranking into pairwise comparisons, a mixed pair contains items from two different groups, the item \"on top\" is said to \"win\" the pair. The per-group metric is the average mixed pairs won by each group, calculated as \\(avgpairs(\\tau, g_i) = \\# ~mixedpairswon(g_i) / \\# totalmixedpairs(g_i)\\) in ranking \\(\\tau\\).  The range of ARP and its \"most fair\" value depends on the per-group aggregation <code>combo</code> variable. </p> <p>Usage: <pre><code>#Calculate ARP with a MaxAbsDiff\nARP, per_group = frt.Metrics.ARP(ranking_df, item_group_dict, 'MaxAbsDiff')\n</code></pre></p> <p>The first returned object is a float specifying the ARP value and the second returned object is a dictionary of FPR scores (average count of won mixed pairs) for each group (keys are group ids).</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{9835646,\n  author={Cachel, Kathleen and Rundensteiner, Elke and Harrison, Lane},\n  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, \n  title={MANI-Rank: Multiple Attribute and Intersectional Group Fairness for Consensus Ranking}, \n  year={2022},\n  volume={},\n  number={},\n  pages={1124-1137},\n  doi={10.1109/ICDE53745.2022.00089}}\n</code></pre>"},{"location":"metrics/#normalized-discounted-kl-divergence-ndkl","title":"Normalized Discounted KL-Divergence (NDKL)","text":"<p>NDKL asseses the representation of groups in dsicrete prefixes of the ranking. It does not considers the relevance or scores associated with items. It aligns with the fairness cocnept of statistical parity and is assess on a single ranking. The NDKL of ranking \\(\\tau\\) with respect to groups \\(G\\) is defined as: \\(\\frac{1}{Z}\\sum^{|X|}_{i = 1}\\frac{1}{log_{2}(i +1 )}d_{KL}(D_{\\tau_i} || D_{X})\\) where \\(d_{KL}(D_{\\tau_i} || D_{X})\\) is the KL-divergence score of the group proportions of the first \\(i\\) positions in \\(\\tau\\) and the group proportions of the item set \\(X\\) and \\(Z = \\sum_{i = 1}^{| \\tau |} \\frac{1}{log_2(i + 1)}\\). NDKL ranges from 0 to \\(\\infty\\), and is most fair at 0.</p> <p>Usage: <pre><code>#Calculate NDKL\nNDKL= frt.Metrics.NDKL(ranking_df, item_group_dict)\n</code></pre> The returned object is a float specifying the NDKL value.</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3292500.3330691,\nauthor = {Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram},\ntitle = {Fairness-Aware Ranking in Search \\&amp; Recommendation Systems with Application to LinkedIn Talent Search},\nyear = {2019},\nisbn = {9781450362016},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3292500.3330691},\ndoi = {10.1145/3292500.3330691},\nbooktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\&amp; Data Mining},\npages = {2221\u20132231},\nnumpages = {11},\nkeywords = {fairness-aware ranking, talent search \\&amp; recommendation systems},\nlocation = {Anchorage, AK, USA},\nseries = {KDD '19}\n}\n</code></pre>"},{"location":"metrics/#inequity-of-amortized-attention-iaa","title":"Inequity of Amortized Attention (IAA)","text":"<p>IAA assess if a series of rankings is individually fair; meaning items are given attention similiar to their relevance. IAA measures the difference, via the \\(L_1\\) norm between the cumulative attention and cumulative relevance of items in the rankings. Whereby the attention of an item \\(x_i\\) in ranking \\(\\tau\\) is \\(attention(\\tau,x_i) = 1 / log_2(\\tau(x_i)+1))\\) and the relevance of an item is a [0-1]-normalized score. IAA is ranges from 0 to \\(\\infty\\), and is most fair at 0.</p> <p>Usage: <pre><code>#Calculate IAA\nIAA = frt.Metrics.IAA(ranking_df, relevance_df)\n</code></pre> Note that the relevance scores associated with the rankings in <code>relevance_df</code> must be between 0 and 1. The returned object is a float specifying the IAA value.</p> <p>Citation:</p> BibTeX <pre><code>@inproceedings{10.1145/3209978.3210063,\nauthor = {Biega, Asia J. and Gummadi, Krishna P. and Weikum, Gerhard},\ntitle = {Equity of Attention: Amortizing Individual Fairness in Rankings},\nyear = {2018},\nisbn = {9781450356572},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3209978.3210063},\ndoi = {10.1145/3209978.3210063},\nbooktitle = {The 41st International ACM SIGIR Conference on Research \\&amp; Development in Information Retrieval},\npages = {405\u2013414},\nnumpages = {10},\nkeywords = {individual fairness, fair ranking, amortized fairness, exposure, algorithmic fairness, attention, position bias},\nlocation = {Ann Arbor, MI, USA},\nseries = {SIGIR '18}\n}\n</code></pre>"},{"location":"rankers/","title":"Rankers","text":""},{"location":"rankers/#fair-ranking-algorithms","title":"Fair Ranking Algorithms","text":"<p><code>FairRankTune</code> provides fair ranking algorithms in the <code>Rankers</code> module. These fair ranking algorithms can be used to re-rank a given ranking with the objective of making the resulting ranking fair. </p>"},{"location":"rankers/#supported-fair-ranking-algorithms","title":"Supported Fair Ranking Algorithms","text":""},{"location":"rankers/#epsilon-greedy-re-ranker","title":"Epsilon-Greedy Re-Ranker","text":"<p>Epsilon-Greedy takes as input a ranking and repeatedly swaps pairs of items so that each item has probability \\(\\epsilon\\) (<code>epsilon</code>) of swapping with a random item below it. It does not require a specific notion of fairness or prior knowledge of group distributions. It does use random swapping, thus it is recommended to set a random seed for reproducibility. To learn more see Feng et al. where it was introduced to improve group fairness.</p> <p>Usage: <pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\nimport random\n\n#Generate a biased (phi = 0) ranking of 1000 items, with two groups of 100 and 900 items each.\ngroup_proportions = np.asarray([.1, .9]) #Array of group proportions\nnum_items = 1000 #1000 items to be in the generated ranking\nphi = 0 #Biased ranking\nr_cnt = 1 #Generate 1 ranking\nranking_df, item_group_dict, scores_df = frt.RankTune.ScoredGenFromGroups(group_proportions,  num_items, phi, r_cnt, 'uniform', seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, item_group_dict, 'MinMaxRatio')\nprint(\"EXP before Epsilon-Greedy: \", EXP_minmax, \"avg_exposures before Epsilon-Greedy: \", avg_exposures_minmax)\n\n\n#Rerank using Epsilon-Greedy\nseed = 2 #For reproducibility\nepsilon = .6 \nreranking_df, item_group_d, reranking_scores = frt.Rankers.EPSILONGREEDY(ranking_df, item_group_dict, scores_df, epsilon, seed)\n\n#Calculate EXP with a MinMaxRatio post Epsilon-Greedy\nEXP, avg_exposures= frt.Metrics.EXP(reranking_df, item_group_d, 'MinMaxRatio')\nprint(\"EXP after Epsilon-Greedy: \", EXP, \"avg_exposures after Epsilon-Greedy: \", avg_exposures)\n</code></pre></p> <p>Output: <pre><code>EXP before Epsilon-Greedy:  0.5420744267551784 avg_exposures before Epsilon-Greedy:  {0: 0.2093867087428094, 1: 0.11350318011191189}\nEXP after Epsilon-Greedy:  0.7689042373241246 avg_exposures after Epsilon-Greedy:  {0: 0.15541589156986096, 1: 0.1194999375755728}\n</code></pre> <code>epsilon</code> must be between \\([0,1]\\) and a <code>seed</code> is passed for reproducibility.</p> <p>Citation:</p> BibTeX <pre><code>@article{Feng_Shah_2022, title={Has CEO Gender Bias Really Been Fixed? Adversarial Attacking and Improving Gender Fairness in Image Search},\nvolume={36},\nurl={https://ojs.aaai.org/index.php/AAAI/article/view/21445},\nDOI={10.1609/aaai.v36i11.21445},\nnumber={11},\njournal={Proceedings of the AAAI Conference on Artificial Intelligence},\nauthor={Feng, Yunhe and Shah, Chirag},\nyear={2022},\nmonth={Jun.},\n pages={11882-11890} }\n</code></pre>"},{"location":"rankers/#detconstsort-re-ranker","title":"DetConstSort Re-Ranker","text":"<p>DetConstSort takes as input a given ranking, and re-ranks items in it to create a top-k fair ranking. Fairness is achieved by setting the <code>distribution</code> dictionary.  In <code>distribution</code> the keys are group identifiers and the value is the desired group proportion. For any particular position k and for any group <code>g</code>, DetConstSort ensures that group occurs \\(\\lfloor\\) <code>distribution[g]</code> \\(*k \\rfloor\\) in the resulting ranking. DetConstSort algorithm also tries improve the utility of the ranking by ensuring that items with better scores are placed higher in the ranking so long as the ranking satisfies the feasibility criteria. To learn more see Geyik et al..</p> <p>Usage: <pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\nimport random\nrandom.seed(10)\n\n#Generate a biased (phi = 0) ranking of 1000 items, with two groups of 100 and 900 items each.\nseed = 2\nranking_df, item_group_dict, scores_df = frt.RankTune.ScoredGenFromGroups(np.asarray([.1, .9]),  1000, 0, 1, 'uniform', seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df, item_group_dict, 'MinMaxRatio')\nprint(\"EXP before DetConstSort: \", EXP_minmax, \"avg_exposures before DetConstSort: \", avg_exposures_minmax)\n\n\n#Rerank using DetConstSort\ndistribution = dict(zip([0, 1], [.1, .9])) #set distribution for statistical parity\nk = 800 #only ranking 800 items of the provided 1000 \nreranking_df, item_group_d, reranking_scores = frt.Rankers.DETCONSTSORT(ranking_df, item_group_dict, scores_df, distribution, k)\n\n#Calculate EXP with a MinMaxRatio post DetConstSort\nEXP, avg_exposures= frt.Metrics.EXP(reranking_df, item_group_d, 'MinMaxRatio')\nprint(\"EXP after DetConstSort: \", EXP, \"avg_exposures after DetConstSort: \", avg_exposures)\n</code></pre></p> <p>Output: <pre><code>EXP before DetConstSort:  0.5420744267551784 avg_exposures before DetConstSort:  {0: 0.2093867087428094, 1: 0.11350318011191189}\nEXP after DetConstSort:  0.9738302276209081 avg_exposures after DetConstSort:  {0: 0.12535449974404395, 1: 0.12872315542133886}\n</code></pre></p> <p>Citation:</p> BibTeX <pre><code>@article{Feng_Shah_2022, title={Has CEO Gender Bias Really Been Fixed? Adversarial Attacking and Improving Gender Fairness in Image Search},\nvolume={36},\nurl={https://ojs.aaai.org/index.php/AAAI/article/view/21445},\nDOI={10.1609/aaai.v36i11.21445},\nnumber={11},\njournal={Proceedings of the AAAI Conference on Artificial Intelligence},\nauthor={Feng, Yunhe and Shah, Chirag},\nyear={2022},\nmonth={Jun.},\n pages={11882-11890} }\n</code></pre>"},{"location":"rankers/#contributing-fair-ranking-algorithms","title":"Contributing Fair Ranking Algorithms","text":"<p>We believe in open-source community driven software. Please reach out to expand the <code>FairRankTune</code> algorithm offerings. </p>"},{"location":"ranktune/","title":"RankTune","text":""},{"location":"ranktune/#overview","title":"Overview","text":"<p>RankTune is a pseudo-stochastic data generation method for creating fairness-aware ranked lists using the fairness concept of statistical parity. Included in the <code>RankTune</code> module, it creates ranking(s) based on the <code>phi</code> representativeness parameter. When <code>phi = 0</code> then the generated ranked list(s) does not represent groups fairly, and as <code>phi</code> increases groups are represented more and more fairly; thus <code>phi = 1</code> groups are fairly represented. RankTune uses a pseudo-random process to generate fairness-aware ranked data. RankTune can generate ranked data from user provided group sizes, from existing datasets, along with producing relevance scores accompanying the ranked list(s). </p>"},{"location":"ranktune/#usage","title":"Usage","text":"<p>RankTune can be utilized through four function interfaces.</p>"},{"location":"ranktune/#using-group-sizes","title":"Using Group Sizes","text":"<p><code>RankTune</code> can be used to generate ranking(s) from <code>group_proportions</code>, a numpy array with each group's proportion of the total items,<code>num_items</code>, by using the <code>GenFromGroups()</code> function.</p> <p>Usage: GenFromGroups() function<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0.1) ranking of 1000 items, with four groups of 100,\n# 200, 300, and 400 items each.\ngroup_proportions = np.asarray([.1, .2, .3, .4]) #Array of group proportions\nnum_items = 1000 #1000 items to be in the generated ranking\nphi = 0.1\nr_cnt = 1 #Generate 1 ranking\nseed = 10 #For reproducibility\nranking_df, item_group_dict = frt.RankTune.GenFromGroups(group_proportions,\n    num_items, phi, r_cnt, seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df,\n    item_group_dict, 'MinMaxRatio')\nprint(\"EXP of generated ranking: \", EXP_minmax,\n    \"avg_exposures: \", avg_exposures_minmax)\n</code></pre></p> <p>Output: <pre><code>EXP of generated ranking:  0.511665941043515 avg_exposures:  {0: 0.20498798214669187, 1: 0.13126425437156242, 2: 0.11461912123646827, 3: 0.10488536878769836}\n</code></pre> Can confirm this is an unfair ranking by the low EXP value.</p>"},{"location":"ranktune/#using-an-existing-dataset","title":"Using an Existing Dataset","text":"<p><code>RankTune</code> can be used to generate ranking(s) from <code>item_group_dict</code>, a dictionary of items where the keys are each item's group by using the <code>GenFromItems()</code> function.</p> <p>Usage: GenFromItems() function<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0.1) ranking\nitem_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\",\n    Jill= \"W\", Jane= \"W\", Dave= \"M\", Nancy= \"W\", Nick= \"M\")\nphi = 0.1\nr_cnt = 1 #Generate 1 ranking\nseed = 10 #For reproducibility\nranking_df, item_group_dict = frt.RankTune.GenFromItems(item_group_dict,\n    phi, r_cnt, seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df,\n    item_group_dict, 'MinMaxRatio')\nprint(\"Generated ranking: \", ranking_df)\nprint(\"EXP of generated ranking: \", EXP_minmax,\n    \"avg_exposures: \", avg_exposures_minmax)\n</code></pre></p> <p>Output: <pre><code>Generated ranking:         0\n0   Nick\n1   Dave\n2  David\n3    Joe\n4  Nancy\n5   Jane\n6   Jill\n7    Amy\n8  Heidi\n9  Bella\nEXP of generated ranking:  0.5158099476966725 avg_exposures:  {'M': 0.6404015779112127, 'W': 0.33032550440724917}\n</code></pre> We can confirm this is a biased ranking base don the low EXP score and large difference in average exposure between the 'M' and 'W' groups.</p>"},{"location":"ranktune/#generating-scores-with-the-ranking","title":"Generating Scores With the Ranking","text":"<p>Both <code>GenFromGroups()</code> and <code>GenFromItems()</code> contain sibling functions; respectively, <code>ScoredGenFromGroups()</code> and <code>ScoredGenFromItems()</code> that also generate relevance scores alongside the produced ranking(s). Scores are generated from either a normal or uniform distribution by setting the <code>score_dist</code> parameter to either <code>normal</code> or <code>uniform</code>.</p> <p>For generating from group proportions use <code>ScoredGenFromGroups()</code> as follows:</p> ScoredGenFromGroups() function<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0.1) ranking of 1000 items, with four groups of 100, \n# 200, 300, and 400 items each.\ngroup_proportions = np.asarray([.1, .2, .3, .4]) #Array of group proportions\nnum_items = 1000 #1000 items to be in the generated ranking\nphi = 0.1\nr_cnt = 1 #Generate 1 ranking\nseed = 11 #For reproducibility\nscore_dist = \"uniform\"\nranking_df, item_group_dict, scores_df = frt.RankTune.ScoredGenFromGroups(group_proportions,\n    num_items, phi, r_cnt, score_dist, seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df,\n    item_group_dict, 'MinMaxRatio')\nprint(\"EXP of generated ranking: \", EXP_minmax,\n    \"avg_exposures: \", avg_exposures_minmax)\n</code></pre> <p>Output: <pre><code>EXP of generated ranking:  0.5218433930014378 avg_exposures:  {0: 0.20212221456603452, 1: 0.13273063123258025, 2: 0.11380909457280128, 3: 0.10547614225010409}\n</code></pre></p> <p>For generating from existing items use <code>ScoredGenFromItems()</code> as follows:</p> ScoredGenFromGroups() function<pre><code>import FairRankTune as frt\nimport numpy as np\nimport pandas as pd\nfrom FairRankTune import RankTune, Metrics\n\n#Generate a biased (phi = 0.1) ranking\nitem_group_dict = dict(Joe= \"M\",  David= \"M\", Bella= \"W\", Heidi= \"W\", Amy = \"W\",\n    Jill= \"W\", Jane= \"W\", Dave= \"M\", Nancy= \"W\", Nick= \"M\")\nr_cnt = 1 #Generate 1 ranking\nseed = 10 #For reproducibility\nphi = 0.1\nscore_dist = \"uniform\"\nranking_df, item_group_dict, score_df = frt.RankTune.ScoredGenFromItems(item_group_dict,\n    phi, r_cnt, score_dist,seed)\n\n#Calculate EXP with a MinMaxRatio\nEXP_minmax, avg_exposures_minmax = frt.Metrics.EXP(ranking_df,\n    item_group_dict, 'MinMaxRatio')\nprint(\"Generated ranking: \", ranking_df)\nprint(\"Generated scores: \", score_df)\nprint(\"EXP of generated ranking: \", EXP_minmax,\n    \"avg_exposures: \", avg_exposures_minmax)\n</code></pre> <p>Output: <pre><code>Generated ranking:         0\n0   Nick\n1   Dave\n2  David\n3    Joe\n4  Nancy\n5   Jane\n6   Jill\n7    Amy\n8  Heidi\n9  Bella\nGenerated scores:            0\n0  0.771321\n1  0.760531\n2  0.748804\n3  0.633648\n4  0.498507\n5  0.224797\n6  0.198063\n7  0.169111\n8  0.088340\n9  0.020752\nEXP of generated ranking:  0.5158099476966725 avg_exposures:  {'M': 0.6404015779112127, 'W': 0.33032550440724917}\n</code></pre></p>"},{"location":"ranktune/#how-does-it-work","title":"How does it work?","text":"<p>RankTune is a fairness-tunable  ranked data generation method. It constructs a ranking(s) <code>ranking_df</code> by placing items into the constructed ranking from top to bottom. The idea behind RankTune is that to construct a \"fair\" ranking, each time we place an item in the generated ranking, the likelihood of placing an item in a given group should be equal to that group's proportion of the total items (i.e., if a group is 20% of the item pool, then it should have a 20% chance of being placed). Then, on the other side of the spectrum, if we want a completely \"unfair\" ranking, we should place items into the rankings such that groups are ordered by increasing size from small to large. In this way, smaller groups  get bigger proportions of favorable positions, which violates statistical parity fairness. </p> <p>To generate rankings along the statistical parity fairness spectrum, RankTune samples a random number in the [0, 1] interval each time it places an item. We design this interval to have \"regions\" that map to groups. In this way, the unfairness tuning parameter <code>phi</code> controls representativeness, i.e.,  how fairly each group is represented in the ranking. Specifically, when <code>phi = 1</code> , then  each group is fairly represented. Thus each group's region is equal to the group's proportion of the pool (fair). As <code>phi</code> decreases, the fair representation of each group degrades because regions are distorted in such a way that smaller groups have larger regions compared to their proportion of the total pool (unfair). The fairness tuning parameter <code>phi</code> is used to create the regions prior to placing any items into the generated ranking.</p>"},{"location":"ranktune/#ranktune-demonstration","title":"RankTune Demonstration","text":"<p>The figure below displays the results of generating 200 rankings for different <code>phi</code> representativeness values for multiple group distributions. Average metric values (with \\(95\\%\\) confidence intervals) are reported for 200 generated rankings per group distribution of \\(1,000\\) items. As <code>phi</code> increases, RankTune outputs increasingly fairer rankings. EXP and AWRF are measured with <code>MinMaxRatio</code> combo variables and are more fair at \\(1\\) (thus upward slopes), and NDKL, EE-D (EXP with an <code>LTwo</code> combo variable ), and ARP (with an <code>MaxMinDiff</code> combo variable) are more fair at \\(0\\) (thus downward slopes). To learn more, including the group distributions used, please see our technical report. </p>"}]}